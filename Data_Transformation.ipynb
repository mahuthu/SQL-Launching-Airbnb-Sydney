{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahuthu/SQL-Launching-Airbnb-Sydney/blob/main/Data_Transformation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sptBPPODrLYn",
        "outputId": "05ea5757-b49b-4e43-9d22-8fddbdee4b5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=ede319ea76db2d987c73a6d7361b3ef9c7c735c05e9435bb2fa2423841a54454\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Data Transformation in PySpark\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "WVurE1osthFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, min, max\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Catherine\", 29)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Calculate Min and Max for Age\n",
        "age_min = df.select(min(col(\"Age\"))).collect()[0][0]\n",
        "age_max = df.select(max(col(\"Age\"))).collect()[0][0]\n",
        "\n",
        "# Perform Normalization\n",
        "df_normalized = df.withColumn(\"Age_Normalized\", (col(\"Age\") - age_min) / (age_max - age_min))\n",
        "\n",
        "df_normalized.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTiGdA9vvIBe",
        "outputId": "9f73d977-7cdf-426c-88e0-1e5c7df941d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+--------------+\n",
            "|     Name|Age|Age_Normalized|\n",
            "+---------+---+--------------+\n",
            "|    Alice| 34|        0.3125|\n",
            "|      Bob| 45|           1.0|\n",
            "|Catherine| 29|           0.0|\n",
            "+---------+---+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean, stddev\n",
        "\n",
        "# Calculate Mean and Standard Deviation for Age\n",
        "age_mean = df.select(mean(col(\"Age\"))).collect()[0][0]\n",
        "age_stddev = df.select(stddev(col(\"Age\"))).collect()[0][0]\n",
        "\n",
        "# Perform Standardization\n",
        "df_standardized = df.withColumn(\"Age_Standardized\", (col(\"Age\") - age_mean) / age_stddev)\n",
        "\n",
        "df_standardized.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zto_VvzHylG_",
        "outputId": "63f86fd3-a727-4bd9-c8e4-e7a1d53f9281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+--------------------+\n",
            "|     Name|Age|    Age_Standardized|\n",
            "+---------+---+--------------------+\n",
            "|    Alice| 34|-0.24433888871261045|\n",
            "|      Bob| 45|   1.099524999206747|\n",
            "|Catherine| 29| -0.8551861104941365|\n",
            "+---------+---+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"red\",), (\"green\",), (\"blue\",)]\n",
        "columns = [\"Color\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Label Encoding\n",
        "indexer = StringIndexer(inputCol=\"Color\", outputCol=\"Color_Index\")\n",
        "df_indexed = indexer.fit(df).transform(df)\n",
        "\n",
        "# One-Hot Encoding\n",
        "encoder = OneHotEncoder(inputCol=\"Color_Index\", outputCol=\"Color_OneHot\")\n",
        "df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
        "\n",
        "df_encoded.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDoqmzKPzFa3",
        "outputId": "6867bcff-5d1d-4e63-c59a-74f757c901e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+-------------+\n",
            "|Color|Color_Index| Color_OneHot|\n",
            "+-----+-----------+-------------+\n",
            "|  red|        2.0|    (2,[],[])|\n",
            "|green|        1.0|(2,[1],[1.0])|\n",
            "| blue|        0.0|(2,[0],[1.0])|\n",
            "+-----+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample DataFrame\n",
        "data = [(\"Apple\",), (\"Banana\",), (\"Cherry\",), (\"Apple\",), (\"Banana\",), (\"Apple\",)]\n",
        "columns = [\"Fruit\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU3RjXxy0N9o",
        "outputId": "d7ac0be7-a950-488f-c080-35d8939bdee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "| Fruit|\n",
            "+------+\n",
            "| Apple|\n",
            "|Banana|\n",
            "|Cherry|\n",
            "| Apple|\n",
            "|Banana|\n",
            "| Apple|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Initialize StringIndexer\n",
        "indexer = StringIndexer(inputCol=\"Fruit\", outputCol=\"Fruit_Index\")\n",
        "\n",
        "# Fit and Transform the DataFrame\n",
        "df_indexed = indexer.fit(df).transform(df)\n",
        "df_indexed.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_eYrPsB0S1m",
        "outputId": "2b988ae2-92c5-4c41-bdf6-2bcb07d15e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+\n",
            "| Fruit|Fruit_Index|\n",
            "+------+-----------+\n",
            "| Apple|        0.0|\n",
            "|Banana|        1.0|\n",
            "|Cherry|        2.0|\n",
            "| Apple|        0.0|\n",
            "|Banana|        1.0|\n",
            "| Apple|        0.0|\n",
            "+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "# Initialize OneHotEncoder\n",
        "encoder = OneHotEncoder(inputCol=\"Fruit_Index\", outputCol=\"Fruit_OneHot\")\n",
        "\n",
        "# Transform the DataFrame\n",
        "df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
        "df_encoded.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Kf_XEfz0zas",
        "outputId": "3f3728bc-97ac-44e5-db37-72e00da62e42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+-------------+\n",
            "| Fruit|Fruit_Index| Fruit_OneHot|\n",
            "+------+-----------+-------------+\n",
            "| Apple|        0.0|(2,[0],[1.0])|\n",
            "|Banana|        1.0|(2,[1],[1.0])|\n",
            "|Cherry|        2.0|    (2,[],[])|\n",
            "| Apple|        0.0|(2,[0],[1.0])|\n",
            "|Banana|        1.0|(2,[1],[1.0])|\n",
            "| Apple|        0.0|(2,[0],[1.0])|\n",
            "+------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#full code\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"Apple\",), (\"Banana\",), (\"Cherry\",), (\"Apple\",), (\"Banana\",), (\"Apple\",)]\n",
        "columns = [\"Fruit\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Label Encoding\n",
        "indexer = StringIndexer(inputCol=\"Fruit\", outputCol=\"Fruit_Index\")\n",
        "df_indexed = indexer.fit(df).transform(df)\n",
        "\n",
        "# One-Hot Encoding\n",
        "encoder = OneHotEncoder(inputCol=\"Fruit_Index\", outputCol=\"Fruit_OneHot\")\n",
        "df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
        "\n",
        "# Show the final DataFrame\n",
        "df_encoded.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i0w6Ofo1Aao",
        "outputId": "f5211e57-bc56-4088-83d7-72bf3bd7f754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+-------------+\n",
            "| Fruit|Fruit_Index| Fruit_OneHot|\n",
            "+------+-----------+-------------+\n",
            "| Apple|        0.0|(2,[0],[1.0])|\n",
            "|Banana|        1.0|(2,[1],[1.0])|\n",
            "|Cherry|        2.0|    (2,[],[])|\n",
            "| Apple|        0.0|(2,[0],[1.0])|\n",
            "|Banana|        1.0|(2,[1],[1.0])|\n",
            "| Apple|        0.0|(2,[0],[1.0])|\n",
            "+------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 34, \"Female\"), (\"Bob\", 45, \"Male\"), (\"Catherine\", 29, \"Female\")]\n",
        "columns = [\"Name\", \"Age\", \"Gender\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg2V3TN22grV",
        "outputId": "7e07a3fb-1e83-4818-ccd6-58355f54c706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+------+\n",
            "|     Name|Age|Gender|\n",
            "+---------+---+------+\n",
            "|    Alice| 34|Female|\n",
            "|      Bob| 45|  Male|\n",
            "|Catherine| 29|Female|\n",
            "+---------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a new feature Age_Squared = Age * Age\n",
        "df_poly = df.withColumn(\"Age_Squared\", col(\"Age\") ** 2)\n",
        "\n",
        "df_poly.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quku1E8i2vZZ",
        "outputId": "b5e13c71-1f3d-4e6f-c37b-05fdc517be57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+------+-----------+\n",
            "|     Name|Age|Gender|Age_Squared|\n",
            "+---------+---+------+-----------+\n",
            "|    Alice| 34|Female|     1156.0|\n",
            "|      Bob| 45|  Male|     2025.0|\n",
            "|Catherine| 29|Female|      841.0|\n",
            "+---------+---+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample DataFrame\n",
        "data = [(1, 10), (2, 20), (3, 30), (4, 40)]\n",
        "columns = [\"Units_Sold\", \"Price_Per_Unit\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Create an interaction feature: Total_Sales_Value = Units_Sold * Price_Per_Unit\n",
        "df_interaction = df.withColumn(\"Total_Sales_Value\", col(\"Units_Sold\") * col(\"Price_Per_Unit\"))\n",
        "\n",
        "df_interaction.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv5qNkQi3DRg",
        "outputId": "c2fb1ae4-a6e0-45ad-b140-dd1714182543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+-----------------+\n",
            "|Units_Sold|Price_Per_Unit|Total_Sales_Value|\n",
            "+----------+--------------+-----------------+\n",
            "|         1|            10|               10|\n",
            "|         2|            20|               40|\n",
            "|         3|            30|               90|\n",
            "|         4|            40|              160|\n",
            "+----------+--------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding Interaction and Polynomial Features in PySpark\n"
      ],
      "metadata": {
        "id": "dd2zXN0TCLpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Understanding Interaction and Polynomial Features\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "vg4cb6-9CQWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(20,), (30,), (40,), (50,)]\n",
        "columns = [\"Age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Create Polynomial Feature: Age_Squared = Age * Age\n",
        "df_poly = df.withColumn(\"Age_Squared\", col(\"Age\") ** 2)\n",
        "\n",
        "df_poly.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxS6tJEVCXTZ",
        "outputId": "f834b417-1527-4429-a616-ea463a4b112a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+\n",
            "|Age|Age_Squared|\n",
            "+---+-----------+\n",
            "| 20|      400.0|\n",
            "| 30|      900.0|\n",
            "| 40|     1600.0|\n",
            "| 50|     2500.0|\n",
            "+---+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample DataFrame\n",
        "data = [(1, 10), (2, 20), (3, 30)]\n",
        "columns = [\"Units_Sold\", \"Price_Per_Unit\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Create Interaction Feature: Total_Sales = Units_Sold * Price_Per_Unit\n",
        "df_interaction = df.withColumn(\"Total_Sales\", col(\"Units_Sold\") * col(\"Price_Per_Unit\"))\n",
        "\n",
        "df_interaction.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc1RkEqKCyao",
        "outputId": "d34bab2e-bf61-4c6f-cec9-76934df64631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+-----------+\n",
            "|Units_Sold|Price_Per_Unit|Total_Sales|\n",
            "+----------+--------------+-----------+\n",
            "|         1|            10|         10|\n",
            "|         2|            20|         40|\n",
            "|         3|            30|         90|\n",
            "+----------+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Sample DataFrame with historical stock data\n",
        "data = [(100, 1000, 0.1), (110, 1100, 0.2), (105, 900, -0.1)]\n",
        "columns = [\"Closing_Price\", \"Volume\", \"Market_Sentiment\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Prepare the data\n",
        "vec_assembler = VectorAssembler(inputCols=columns, outputCol=\"Features\")\n",
        "df_vector = vec_assembler.transform(df)\n",
        "\n",
        "# Initialize Linear Regression Model\n",
        "lr = LinearRegression(featuresCol=\"Features\", labelCol=\"Closing_Price\")\n",
        "\n",
        "# Fit the model\n",
        "lr_model = lr.fit(df_vector)\n",
        "\n",
        "# Make predictions\n",
        "predictions = lr_model.transform(df_vector)\n",
        "predictions.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp3AhmWPGl7f",
        "outputId": "24dc3ef6-37b4-4f44-b38a-611e0d445cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------+----------------+------------------+------------------+\n",
            "|Closing_Price|Volume|Market_Sentiment|          Features|        prediction|\n",
            "+-------------+------+----------------+------------------+------------------+\n",
            "|          100|  1000|             0.1|[100.0,1000.0,0.1]| 99.99999999999952|\n",
            "|          110|  1100|             0.2|[110.0,1100.0,0.2]|110.00000000000028|\n",
            "|          105|   900|            -0.1|[105.0,900.0,-0.1]|105.00000000000016|\n",
            "+-------------+------+----------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}